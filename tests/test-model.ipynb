{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os,sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..', 'module'))\n",
    "import wiki\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "### Condensed sparse column matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "row = np.array([0, 2, 2, 0, 1, 2])\n",
    "col = np.array([0, 0, 1, 2, 2, 2])\n",
    "sp.sparse.csc_matrix((data, (row, col)), shape=(3, 3)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# topics = ['anatomy', 'biochemistry', 'cognitive science', 'evolutionary biology',\n",
    "#           'genetics', 'immunology', 'molecular biology', 'chemistry', 'biophysics',\n",
    "#           'energy', 'optics', 'earth science', 'geology', 'meteorology']\n",
    "topics = ['earth science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_saved = '/Users/harangju/Developer/data/wiki/graphs/dated/'\n",
    "networks = {}\n",
    "for topic in topics:\n",
    "    print(topic, end=' ')\n",
    "    networks[topic] = wiki.Net()\n",
    "    networks[topic].load_graph(path_saved + topic + '.pickle')\n",
    "graph = networks[topic].graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(networks[topic].graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v = networks[topic].graph.graph['tfidf']\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v[:,0].indices[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v[4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "networks[topic].graph.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "networks[topic].graph.nodes['Biology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "core = [n for n in networks[topic].graph.nodes if networks[topic].graph.nodes[n]['core_rb']>.9]\n",
    "core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[(i,n) for i,n in enumerate(networks[topic].graph.nodes) if networks[topic].graph.nodes[n]['year']<-1800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vi = v[:,9]\n",
    "vi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSC & networkx operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph = networks[topic].graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "core = [n for n in networks[topic].graph.nodes if networks[topic].graph.nodes[n]['year']<-2000]\n",
    "subgraph = graph.subgraph(core).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tfidf = ss.hstack([v[:,list(graph.nodes).index(n)] for n in subgraph.nodes])\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subgraph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subgraph.add_node('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subgraph.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "Initialize with core set of nodes.\\\n",
    "For each year,\\\n",
    "initialize an \"baby\" node for each existing node that doesn't already have a baby node,\\\n",
    "mutate tf-idf for each \"baby\" node (including the name),\\\n",
    "and if the \"baby\" node gets a probability drawn from the distribution of similarities (to what?),\n",
    "the \"baby\" node is born."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics.pairwise as smp\n",
    "import scipy.sparse as ss\n",
    "from scipy.stats import norm\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: power law distributions of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = networks[topic].graph\n",
    "tfidf = graph.graph['tfidf'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import powerlaw\n",
    "fit = powerlaw.Fit(tfidf[:,1].data)\n",
    "fit.plot_pdf()\n",
    "fit.power_law.plot_pdf();\n",
    "plt.title(f\"xmin={fit.xmin:.1e}, α={fit.alpha:.1f}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: new words / year between neighbors\n",
    "[gist](https://gist.github.com/ptocca/e18a9e4e35930c0958fdaa62958bdf82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def year_diffs(graph):\n",
    "    return [graph.nodes[node]['year'] - graph.nodes[neighbor]['year']\n",
    "            for node in graph.nodes\n",
    "            for neighbor in list(graph.successors(node))]\n",
    "\n",
    "yd = year_diffs(graph)\n",
    "sns.distplot(yd)\n",
    "plt.title(topic)\n",
    "plt.xlabel('year difference');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%cython -f\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from cython cimport floating,boundscheck,wraparound\n",
    "from cython.parallel import prange\n",
    "\n",
    "from libc.math cimport fabs\n",
    "\n",
    "np.import_array()\n",
    "\n",
    "@boundscheck(False)  # Deactivate bounds checking\n",
    "@wraparound(False)\n",
    "def cython_manhattan(floating[::1] X_data, int[:] X_indices, int[:] X_indptr,\n",
    "                     floating[::1] Y_data, int[:] Y_indices, int[:] Y_indptr,\n",
    "                     double[:, ::1] D):\n",
    "    \"\"\"Pairwise L1 distances for CSR matrices.\n",
    "    Usage:\n",
    "    >>> D = np.zeros(X.shape[0], Y.shape[0])\n",
    "    >>> cython_manhattan(X.data, X.indices, X.indptr,\n",
    "    ...                  Y.data, Y.indices, Y.indptr,\n",
    "    ...                  D)\n",
    "    \"\"\"\n",
    "    cdef np.npy_intp px, py, i, j, ix, iy\n",
    "    cdef double d = 0.0\n",
    "    \n",
    "    cdef int m = D.shape[0]\n",
    "    cdef int n = D.shape[1]\n",
    "    \n",
    "    with nogil:                          \n",
    "        for px in prange(m):\n",
    "            for py in range(n):\n",
    "                i = X_indptr[px]\n",
    "                j = Y_indptr[py]\n",
    "                d = 0.0\n",
    "                while i < X_indptr[px+1] and j < Y_indptr[py+1]:\n",
    "                    if i < X_indptr[px+1]: ix = X_indices[i]\n",
    "                    if j < Y_indptr[py+1]: iy = Y_indices[j]\n",
    "                    \n",
    "                    if ix==iy:\n",
    "                        d = d+fabs(X_data[i]-Y_data[j])\n",
    "                        i = i+1\n",
    "                        j = j+1\n",
    "                    \n",
    "                    elif ix<iy:\n",
    "                        d = d+fabs(X_data[i])\n",
    "                        i = i+1\n",
    "                    else:\n",
    "                        d = d+fabs(Y_data[j])\n",
    "                        j = j+1\n",
    "                \n",
    "                if i== X_indptr[px+1]:\n",
    "                    while j < Y_indptr[py+1]:\n",
    "                        iy = Y_indices[j]\n",
    "                        d = d+fabs(Y_data[j])\n",
    "                        j = j+1                                            \n",
    "                else:\n",
    "                    while i < X_indptr[px+1]:\n",
    "                        ix = X_indices[i]\n",
    "                        d = d+fabs(X_data[i])\n",
    "                        i = i+1\n",
    "                        \n",
    "                D[px,py] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as skp\n",
    "import sklearn.metrics.pairwise as smp\n",
    "from scipy.sparse import csr_matrix,random\n",
    "from sklearn.metrics.pairwise import check_pairwise_arrays\n",
    "\n",
    "def sparse_manhattan(X,Y=None):\n",
    "    X, Y = check_pairwise_arrays(X, Y)\n",
    "    X = csr_matrix(X, copy=False)\n",
    "    Y = csr_matrix(Y, copy=False)\n",
    "    res = np.empty(shape=(X.shape[0],Y.shape[0]))\n",
    "    cython_manhattan(X.data,X.indices,X.indptr,\n",
    "                     Y.data,Y.indices,Y.indptr,\n",
    "                             res)\n",
    "    return res\n",
    "\n",
    "def word_diffs(graph, tfidf):\n",
    "    dists = sparse_manhattan(X=skp.binarize(tfidf).transpose())\n",
    "    nodes = list(graph.nodes)\n",
    "    return [dists[nodes.index(node), nodes.index(neighbor)]\n",
    "            for node in nodes\n",
    "            for neighbor in list(graph.successors(node))]\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(121)\n",
    "wd = word_diffs(graph, tfidf)\n",
    "sns.scatterplot(x=np.abs(yd), y=wd)\n",
    "slope, intercept, r, p, stderr = sp.stats.linregress(np.abs(yd), wd)\n",
    "x = np.linspace(0, max(yd), 100)\n",
    "sns.lineplot(x, np.multiply(slope, x) + intercept)\n",
    "plt.title(f\"slope={slope:.2f}; r={r:.2f}; p={p:.1e}\")\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('manhattan distance');\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.distplot(wd)\n",
    "mu, std = sp.stats.norm.fit(wd)\n",
    "x = np.linspace(min(wd), max(wd), 100)\n",
    "plt.plot(x, sp.stats.norm.pdf(x, mu, std))\n",
    "plt.xlabel('manhattan distance')\n",
    "plt.ylabel('probability distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: similarity / year between neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbor_similarity(graph, tfidf):\n",
    "    nodes = list(graph.nodes)\n",
    "    return [smp.cosine_similarity(tfidf[:,nodes.index(node)].transpose(),\n",
    "                                  tfidf[:,nodes.index(neighbor)].transpose())[0,0]\n",
    "            for node in nodes\n",
    "            for neighbor in list(graph.successors(node))]\n",
    "\n",
    "neighbors = neighbor_similarity(graph, tfidf)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x=np.abs(yd), y=neighbors)\n",
    "slope, intercept, r, p, stderr = sp.stats.linregress(np.abs(yd), neighbors)\n",
    "x = np.linspace(0, max(yd), 100)\n",
    "sns.lineplot(x, np.multiply(slope, x) + intercept)\n",
    "plt.title(f\"slope={slope:.2f}; r={r:.2f}; p={p:.1e}\")\n",
    "plt.xlabel('Δyear')\n",
    "plt.ylabel('cosine similarity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: weight distributions of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_distribution(data):\n",
    "    bins = np.logspace(np.log10(min(data)), np.log10(max(data)), 50)\n",
    "    hist, edges = np.histogram(data, bins=bins)\n",
    "#     hist_norm = hist/(bins[1:] - bins[:-1])\n",
    "    sns.scatterplot(bins[:-1], hist/len(data))\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.xlim(bins[0]/2, bins[-1]*2)\n",
    "    plt.ylim(min(hist[hist>0])/len(data)/2, 1)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('P(x)')\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "sns.scatterplot(x='index', y='weight',\n",
    "                data=pd.DataFrame({'index': tfidf.indices,\n",
    "                                   'weight': tfidf.data}))\n",
    "sns.scatterplot(x='index', y='weight',\n",
    "                data=pd.DataFrame({'index': tfidf.indices,\n",
    "                                   'weight': tfidf.data})\\\n",
    "                       .groupby('index').mean()\\\n",
    "                       .reset_index())\n",
    "plt.ylim([-.2,1.2]);\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_distribution(tfidf.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: year distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot([graph.nodes[node]['year'] for node in graph.nodes], rug=True)\n",
    "plt.xlabel('year');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy.random as npr\n",
    "\n",
    "def mutate(x, rvs, point=(0,0), insert=(0,0,None), delete=(0,0)):\n",
    "    \"\"\" Mutates vector ``x`` with point mutations,\n",
    "    insertions, and deletions. Insertions and point\n",
    "    mutations draw from a random process ``rvs``.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: spipy.sparse.csc_matrix\n",
    "    rvs: lambda ()-> float\n",
    "        returns a random weight, [0,1]\n",
    "    point: tuple (int n, float p)\n",
    "        n = number of elements to insert\n",
    "        p = probability of insertion for each trial\n",
    "    insert: tuple (n, p, iterable s)\n",
    "        s = set of elements from which to select\n",
    "            if None, select from all zero elements\n",
    "    delete: tuple (n, p)\n",
    "    \"\"\"\n",
    "    data = x.data\n",
    "    idx = x.indices\n",
    "    for _ in range(point[0]):\n",
    "        if npr.rand() < point[1]:\n",
    "            data[npr.choice(x.size)] = rvs()\n",
    "    for _ in range(insert[0]):\n",
    "        if npr.rand() < insert[1]:\n",
    "            while True:\n",
    "                insert_idx = npr.choice(insert[2]) if insert[2]\\\n",
    "                    else npr.choice(x.shape[0])\n",
    "                if insert_idx not in idx: break\n",
    "            idx = np.append(idx, insert_idx)\n",
    "            data = np.append(data, rvs())\n",
    "    for _ in range(delete[0]):\n",
    "        if npr.rand() < delete[1]:\n",
    "            delete_idx = npr.choice(idx.size)\n",
    "            idx = np.delete(idx, delete_idx)\n",
    "            data = np.delete(data, delete_idx)\n",
    "    y = ss.csc_matrix((data, (idx, np.zeros(idx.shape, dtype=int))),\n",
    "                      shape=x.shape)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = tfidf[:,1].copy()\n",
    "y = tfidf[:,1].copy()\n",
    "T = 300\n",
    "\n",
    "sim = np.zeros(T)\n",
    "size = np.zeros(T)\n",
    "for i in range(sim.size):\n",
    "    sim[i] = smp.cosine_similarity(x.transpose(),y.transpose())[0,0]\n",
    "    size[i] = y.size\n",
    "    y = mutate(y, lambda: fit.power_law.generate_random()[0],\n",
    "               point=(10,.5), insert=(10,.5,None), delete=(10,.5))\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(121)\n",
    "sn.lineplot(x=range(sim.size), y=sim)\n",
    "plt.title(graph.name)\n",
    "plt.ylabel('similarity')\n",
    "plt.xlabel('years');\n",
    "plt.subplot(122)\n",
    "sn.lineplot(x=range(sim.size), y=size)\n",
    "plt.title(graph.name)\n",
    "plt.ylabel('size')\n",
    "plt.xlabel('years');\n",
    "\n",
    "plt.figure()\n",
    "plot_distribution(graph.graph['tfidf'][:,1].data)\n",
    "plt.xlabel('tf-idf values');\n",
    "plot_distribution(y.data)\n",
    "plt.title(graph.name)\n",
    "plt.legend(['before mutation', 'after mutation'])\n",
    "plt.xlabel('tf-idf values');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(121)\n",
    "plot_distribution(x.data)\n",
    "plot_distribution(y.data)\n",
    "plt.legend(['before','after']);\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_distribution(x.data)\n",
    "plot_distribution(y.data)\n",
    "plt.yscale('linear')\n",
    "plt.xscale('linear')\n",
    "plt.ylim([0,.2])\n",
    "plt.xlim([0,.1])\n",
    "plt.legend(['before','after']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new nodes\n",
    "\n",
    "#### Prior: distribution of similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_neighbor_similarity(graph, tfidf):\n",
    "    nodes = list(graph.nodes)\n",
    "    sim = [smp.cosine_similarity(tfidf[:,nodes.index(n1)].transpose(),\n",
    "                                 tfidf[:,nodes.index(n2)].transpose())[0,0]\n",
    "           for n1 in graph.nodes\n",
    "           for n2 in graph.nodes\n",
    "           if (n2 is not n1) and (n2 not in list(graph.neighbors(n1)))]\n",
    "    return sim\n",
    "\n",
    "non_neighbors = non_neighbor_similarity(graph, tfidf)\n",
    "\n",
    "plt.figure()\n",
    "sns.distplot(neighbors)\n",
    "x = np.linspace(min(neighbors), max(neighbors), 100)\n",
    "mu, std = sp.stats.norm.fit(neighbors)\n",
    "plt.plot(x, sp.stats.norm.pdf(x, mu, std))\n",
    "sns.distplot(non_neighbors)\n",
    "plt.title(topic)\n",
    "plt.legend([f\"fit-neighbors (m={mu:.2f}; s={std:.2f})\", 'neighbors', 'non-neighbors'])\n",
    "plt.xlabel('cos similarity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method\n",
    "\n",
    "Just draw from normal pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "npr.normal(loc=mu, scale=std, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossover\n",
    "\n",
    "What prior should I use? It needs to be more similar than neighbors. Some kind of a t-test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: maybe just 3 std above mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mu + 3*std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method\n",
    "\n",
    "average? or combine elements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def crossover(v1, v2):\n",
    "    \"\"\" Crosses two vectors by combining half of one\n",
    "    and half of the other.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    v1, v2: scipy.sparse.matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    v3: scipy.sparse.matrix\n",
    "    \"\"\"\n",
    "    idx1 = npr.choice(v1.size, size=int(v1.size/2))\n",
    "    idx2 = npr.choice(v2.size, size=int(v2.size/2))\n",
    "    data = np.array([v1.data[i] for i in idx1] +\n",
    "                    [v2.data[i] for i in idx2])\n",
    "    idx = np.array([v1.indices[i] for i in idx1] +\n",
    "                   [v2.indices[i] for i in idx2])\n",
    "    v3 = ss.csc_matrix((data, (idx, np.zeros(idx.shape,\n",
    "                                             dtype=int))),\n",
    "                       shape=v1.shape)\n",
    "    return v3\n",
    "\n",
    "def crossover_seeds(seeds, threshold=.7):\n",
    "    \"\"\" Crosses ``seeds`` if similarity between two seeds\n",
    "    is greater than ``threshold``. Then, it sets one of the\n",
    "    seeds to ``None``.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seeds: dict {str: scipy.sparse.csc_matrix}\n",
    "    threshold: float\n",
    "    \"\"\"\n",
    "    nodes = list(seeds.keys())\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i+1,len(nodes)):\n",
    "            if nodes[i] not in seeds.keys() or nodes[j] not in seeds.keys():\n",
    "                continue\n",
    "            similarity = smp.cosine_similarity(seeds[nodes[i]].transpose(),\n",
    "                                               seeds[nodes[j]].transpose())\n",
    "            if similarity[0,0] > threshold:\n",
    "                seeds[nodes[i]] = crossover(seeds[nodes[i]], seeds[nodes[j]])\n",
    "                seeds.pop(nodes[j], None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf = graph.graph['tfidf'].copy()\n",
    "nodes = list(graph.nodes)[:6]\n",
    "seeds = {node: tfidf[:,list(graph.nodes).index(node)]\n",
    "         for node in nodes}\n",
    "seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectors = ss.hstack([seeds[node] for node in nodes])\n",
    "print(np.triu(smp.cosine_similarity(vectors.transpose())))\n",
    "crossover_seeds(seeds, threshold=0.5)\n",
    "print('----------------------------------------------------------')\n",
    "vectors = ss.hstack([seeds[node] for node in nodes if node in seeds.keys()])\n",
    "print(np.triu(smp.cosine_similarity(vectors.transpose())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def consume_seeds(seeds, vectors, threshold=0.9):\n",
    "    \"\"\" Consumes a seed in ``seeds`` if similarity\n",
    "    between a seed and an existing vector in ``vectors``\n",
    "    is greater than ``threshold``.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seeds: dict {string: scipy.sparse.csc_matrix}\n",
    "    vectors: scipy.sparse.csc_matrix\n",
    "    threshold: float\n",
    "    \"\"\"\n",
    "    for seed, vec in list(seeds.items()):\n",
    "        for i in range(vectors.shape[1]):\n",
    "            s = smp.cosine_similarity(vec.transpose(), vectors[:,i].transpose())\n",
    "            if s[0,0] > threshold:\n",
    "                seeds.pop(seed, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf = graph.graph['tfidf'].copy()\n",
    "nodes = list(graph.nodes)[:4]\n",
    "seeds = {node: tfidf[:,list(graph.nodes).index(node)]\n",
    "         for node in nodes}\n",
    "T = 100\n",
    "seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    seeds['Hydrosphere'] = mutate(seeds['Hydrosphere'],\n",
    "                                  lambda: fit.power_law.generate_random()[0],\n",
    "                                  point=(1,1), insert=(5,.5,None), delete=(5,.5))\n",
    "consume_seeds(seeds, tfidf[:,:4])\n",
    "seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get words from tf-idf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim.utils as gu\n",
    "\n",
    "path_models = '/Users/harangju/Developer/data/wiki/models/'\n",
    "model = gu.SaveLoad.load(path_models + 'tfidf.model')\n",
    "dct = pickle.load(open(path_models + 'dict.model','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = [dct[i] for i in tfidf[:,0].indices]\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior: word weight vs title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = np.argsort(tfidf[:,0].data)\n",
    "idx[-5:], tfidf[:,0].data[idx[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_top_words(x, dct, top_n=5, stoplist=set('for a of the and to in'.split())):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: scipy.sparse.csc_matrix\n",
    "    dct: gensim.corpora.dictionary\n",
    "    top_n: int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    words:\n",
    "    idx_vector: \n",
    "    \"\"\"\n",
    "    top_idx = np.argsort(x.data)[-top_n:]\n",
    "    idx = [x.indices[i] for i in top_idx if dct[x.indices[i]] not in stoplist]\n",
    "    words = [dct[i] for i in idx]\n",
    "    return words, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stoplist=set('for a of the and to in'.split())\n",
    "nodes = []\n",
    "words1 = []\n",
    "words2 = []\n",
    "for i in range(tfidf.shape[1]):\n",
    "    if tfidf[:,i].data.size == 0:\n",
    "        print(list(graph.nodes)[i], tfidf[:,i].data)\n",
    "        continue\n",
    "    nodes += [list(graph.nodes)[i]]\n",
    "    idx_sorted = np.argsort(tfidf[:,i].data)\n",
    "    words1 += [[dct[tfidf[:,i].indices[idx]]\n",
    "                for idx in idx_sorted[-5:]\n",
    "                if dct[tfidf[:,i].indices[idx]] not in stoplist]]\n",
    "    top_words, idx = find_top_words(tfidf[:,i], dct, top_n=5)\n",
    "    words2 += [top_words]\n",
    "pd.DataFrame(data={'Node': nodes, 'Top words 1': words1, 'Top words 2': words2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method\n",
    "\n",
    "If article has any two of the top five words, connect.\n",
    "```\n",
    "for new_article in new_articles:\n",
    "    for article in articles:\n",
    "        if any two of the top five words are in new_article:\n",
    "            connect new_article to article\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def connect(seed_vector, graph, vectors, dct, top_words=5, match_n=2):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed_vector: scipy.sparse.csc_matrix\n",
    "    graph: networkx.DiGraph (not optional)\n",
    "    vectors: scipy.sparse.csc_matrix (not optional)\n",
    "    dct: gensim.corpora.dictionary (not optional)\n",
    "    top_words: int (default=5)\n",
    "    match_n: int\n",
    "        how many words should be matched by...\n",
    "    \"\"\"\n",
    "    seed_top_words, seed_top_idx = find_top_words(seed_vector, dct)\n",
    "    seed_name = ' '.join(seed_top_words)\n",
    "    nodes = list(graph.nodes)\n",
    "    graph.add_node(seed_name)\n",
    "    for i, node in enumerate(nodes):\n",
    "        node_vector = vectors[:,i]\n",
    "        node_top_words, node_top_idx = find_top_words(node_vector, dct)\n",
    "        if len(set(seed_top_idx).intersection(set(node_vector.indices))) >= match_n:\n",
    "            graph.add_edge(node, seed_name)\n",
    "        if len(set(node_top_idx).intersection(set(seed_vector.indices))) >= match_n:\n",
    "            graph.add_edge(seed_name, node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph = networks[topic].graph\n",
    "core_nodes = [n for n in graph.nodes if graph.nodes[n]['year'] < -2000]\n",
    "subgraph = graph.subgraph(core_nodes).copy()\n",
    "subgraph.graph.clear()\n",
    "subgraph.name = graph.name + '-cutting'\n",
    "print(f\"Core nodes: {core_nodes} in '{subgraph.name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_graph = subgraph.copy()\n",
    "test_vector = ss.hstack([tfidf[:,list(graph.nodes).index(n)] for n in test_graph.nodes])\n",
    "\n",
    "seed = 'Meteorology'\n",
    "seed_vector = tfidf[:,list(graph.nodes).index(seed)]\n",
    "\n",
    "print('Nodes:', test_graph.nodes)\n",
    "print('Edges:', test_graph.edges, '\\n')\n",
    "print(f\"Seed: {seed}\\n\")\n",
    "connect(seed_vector, test_graph, test_vector, dct, match_n=3)\n",
    "print('Nodes:', test_graph.nodes)\n",
    "print('Edges:', test_graph.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with all nodes without node names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import powerlaw\n",
    "# fit = powerlaw.Fit(graph.graph['tfidf'].data)\n",
    "fit.plot_pdf()\n",
    "fit.power_law.plot_pdf();\n",
    "plt.title(f\"Power law x_min={fit.xmin:.1e}, α={fit.alpha:.1f}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=np.abs(yd), y=wd)\n",
    "slope, intercept, fit_r, p, stderr = sp.stats.linregress(np.abs(yd), wd)\n",
    "x = np.linspace(0, max(yd), 100)\n",
    "sns.lineplot(x, np.multiply(slope, x) + intercept)\n",
    "plt.title(f\"slope={slope:.2f}; r={fit_r:.2f}; p={p:.1e}\")\n",
    "plt.xlabel('Δyear')\n",
    "plt.ylabel('manhattan distance (# different words)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neighbors = neighbor_similarity(graph, tfidf)\n",
    "fit_mu, fit_std = sp.stats.norm.fit(neighbors)\n",
    "non_neighbors = non_neighbor_similarity(graph, tfidf)\n",
    "sns.distplot(neighbors)\n",
    "x = np.linspace(min(neighbors), max(neighbors), 100)\n",
    "plt.plot(x, sp.stats.norm.pdf(x, fit_mu, fit_std))\n",
    "sns.distplot(non_neighbors)\n",
    "plt.title(topic)\n",
    "plt.legend([f\"fit-neighbors (m={fit_mu:.2f}; s={fit_std:.2f})\", 'neighbors', 'non-neighbors'])\n",
    "plt.xlabel('cos similarity');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_mu + 3*fit_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method\n",
    "1. Initialize a bag of seeds from a set of nodes.\n",
    "2. For each year,\n",
    "    1. Mutate seeds. For each seed,\n",
    "        1. Change a word with `p_point`. Draw weight from power law prior.\n",
    "        2. Delete a word with `p_delete`.\n",
    "        3. Insert new word with `p_insert`. Draw weight from power law prior.\n",
    "    2. Crossover seeds if `μ+3σ < similarity`.\n",
    "    3. Create new node from seed if `x < similarity` where `x~Norm(θ)`.\n",
    "        1. Connect new node.\n",
    "        2. Initialize new seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def initialize_seeds(seeds, graph, vectors, thresholds, neighbor_stats):\n",
    "    for node in graph.nodes:\n",
    "        if node not in seeds.keys():\n",
    "            seeds[node] = vectors[:,list(graph.nodes).index(node)].copy()\n",
    "            thresholds[node] = npr.normal(loc=neighbor_stats[0],\n",
    "                                          scale=neighbor_stats[1])\n",
    "\n",
    "def mutate_seeds(seeds, rvs, point, insert, delete):\n",
    "    for node, vec in seeds.items():\n",
    "        seeds[node] = mutate(vec, rvs, point=point, insert=insert, delete=delete)\n",
    "\n",
    "def create_nodes(seeds, graph, vectors, thresholds, year):\n",
    "    nodes = list(graph.nodes)\n",
    "    for node in nodes:\n",
    "        sim_to_parent = smp.cosine_similarity(seeds[node].transpose(),\n",
    "                                              vectors[:,nodes.index(node)].transpose())\n",
    "        if sim_to_parent[0,0] < thresholds[node]:\n",
    "            connect(seeds[node], graph, vectors, dct, match_n=3)\n",
    "            vectors = ss.hstack([vectors, seeds[node]])\n",
    "            seeds.pop(node)\n",
    "    for node in graph.nodes:\n",
    "        if 'year' not in graph.nodes[node].keys():\n",
    "            graph.nodes[node]['year'] = year\n",
    "    return vectors\n",
    "\n",
    "def evolve(graph, vectors, year_end, rvs, point, insert, delete, neighbor_stats):\n",
    "    \"\"\" Evolves a graph based on vector representations\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    graph: networkx.DiGraph\n",
    "    vectors: scipy.sparse.csc_matrix\n",
    "    year_end: int (default=2020)\n",
    "    rvs: lambda: float\n",
    "    point, insert, delete: tuple\n",
    "        See ``mutate()``.\n",
    "    neighbor_stats: tuple of floats\n",
    "        (mu, std)\n",
    "    \"\"\"\n",
    "    year_start = max([graph.nodes[n]['year'] for n in graph.nodes])+1\n",
    "    seeds = {}\n",
    "    thresholds = {}\n",
    "    data = pd.DataFrame()\n",
    "    for year in range(year_start, year_end+1):\n",
    "        sys.stdout.write('\\r                                     ')\n",
    "        sys.stdout.write(f\"\\r{year_start}\\t> {year}\\t> {year_end}\")\n",
    "        sys.stdout.flush()\n",
    "        initialize_seeds(seeds, graph, vectors, thresholds, neighbor_stats)\n",
    "        mutate_seeds(seeds, rvs, point=point, insert=insert, delete=delete)\n",
    "        vectors = create_nodes(seeds, graph, vectors, thresholds, year)\n",
    "        crossover_seeds(seeds, neighbor_stats[0]+3*neighbor_stats[1])\n",
    "        for seed, vector in seeds.items():\n",
    "            data = data.append({'Year': year,\n",
    "                                'Parent': seed,\n",
    "                                'Seed vectors': vector},\n",
    "                               ignore_index=True)\n",
    "    return vectors, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_year = -500\n",
    "core_nodes = [n for n in graph.nodes if graph.nodes[n]['year'] < start_year]\n",
    "subgraph = graph.subgraph(core_nodes).copy()\n",
    "subgraph.graph.clear()\n",
    "tfidf = graph.graph['tfidf']\n",
    "vectors = ss.hstack([tfidf[:,list(graph.nodes).index(n)] for n in core_nodes])\n",
    "print(f\"Topic: '{graph.name}'\" +\\\n",
    "      f\"Core nodes: {core_nodes}\" +\\\n",
    "      f\"Parameters:\\tα (power law): {fit.alpha:.2f}\\n\\t\\t\" +\\\n",
    "      f\"p_insert/delete: {fit_r:.2f}\\n\\t\\t\" +\\\n",
    "      f\"neighbor_mu, std: {fit_mu:.2f}, {fit_std:.2f}\\n\\t\\t\" +\\\n",
    "      f\"threshold: {fit_mu+3*fit_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, data = evolve(subgraph, vectors,\n",
    "                       year_end=2000,\n",
    "                       rvs=lambda: fit.power_law.generate_random()[0],\n",
    "                       point=(1,.5),\n",
    "                       insert=(1,r,list(set(tfidf.indices))),\n",
    "                       delete=(1,r),\n",
    "                       neighbor_stats=(fit_mu,fit_std))\n",
    "print('\\n'+repr(vectors))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(121)\n",
    "sns.distplot(neighbors)\n",
    "x = np.linspace(min(neighbors), max(neighbors), 100)\n",
    "mu, std = sp.stats.norm.fit(neighbors)\n",
    "plt.plot(x, sp.stats.norm.pdf(x, mu, std))\n",
    "sns.distplot(non_neighbors)\n",
    "plt.title(topic + ' (prior)')\n",
    "plt.legend([f\"fit-neighbors (m={mu:.2f}; s={std:.2f})\", 'neighbors', 'non-neighbors'])\n",
    "plt.xlabel('cos similarity');\n",
    "plt.xlim([-.2,1.2])\n",
    "plt.subplot(122)\n",
    "neighbors_model = neighbor_similarity(subgraph, vectors)\n",
    "non_neighbors_model = non_neighbor_similarity(subgraph, vectors)\n",
    "sns.distplot(neighbors_model)\n",
    "x = np.linspace(min(neighbors_model), max(neighbors_model), 100)\n",
    "mu, std = sp.stats.norm.fit(neighbors_model)\n",
    "plt.plot(x, sp.stats.norm.pdf(x, mu, std))\n",
    "sns.distplot(non_neighbors_model)\n",
    "plt.title(topic + ' (model)')\n",
    "plt.legend([f\"fit-neighbors (m={mu:.2f}; s={std:.2f})\", 'neighbors', 'non-neighbors'])\n",
    "plt.xlabel('cos similarity')\n",
    "plt.xlim([-.2,1.2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(121)\n",
    "fit.plot_pdf()\n",
    "fit.power_law.plot_pdf()\n",
    "plt.title(f\"empirical xmin={fit.xmin:.1e}, α={fit.alpha:.1f}\");\n",
    "plt.subplot(122)\n",
    "fit_model = powerlaw.Fit(vectors.data)\n",
    "fit_model.plot_pdf()\n",
    "fit_model.power_law.plot_pdf()\n",
    "plt.title(f\"model xmin={fit_model.xmin:.1e}, α={fit_model.alpha:.1f}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(121)\n",
    "sns.distplot(yd)\n",
    "plt.title(topic + ' prior')\n",
    "plt.xlabel('year difference')\n",
    "plt.subplot(122)\n",
    "yd_model = year_diffs(subgraph)\n",
    "sns.distplot(yd_model)\n",
    "plt.title(topic + ' model')\n",
    "plt.xlabel('year difference');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "plt.subplot(221)\n",
    "sns.scatterplot(x=np.abs(yd), y=wd)\n",
    "slope, intercept, r, p, stderr = sp.stats.linregress(np.abs(yd), wd)\n",
    "x = np.linspace(0, max(yd), 100)\n",
    "sns.lineplot(x, np.multiply(slope, x) + intercept)\n",
    "plt.title(f\"slope={slope:.2f}; r={r:.2f}; p={p:.1e} (prior)\")\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('manhattan distance');\n",
    "\n",
    "plt.subplot(222)\n",
    "sns.distplot(wd)\n",
    "mu, std = sp.stats.norm.fit(wd)\n",
    "x = np.linspace(min(wd), max(wd), 100)\n",
    "plt.plot(x, sp.stats.norm.pdf(x, mu, std))\n",
    "plt.xlabel('manhattan distance')\n",
    "plt.ylabel('probability distribution');\n",
    "plt.title(f\"μ={mu:.2}, σ={std:.2} (prior)\")\n",
    "\n",
    "wd_model = word_diffs(subgraph, vectors)\n",
    "\n",
    "plt.subplot(223)\n",
    "sns.scatterplot(x=np.abs(yd_model), y=wd_model)\n",
    "slope, intercept, r, p, stderr = sp.stats.linregress(np.abs(yd_model), wd_model)\n",
    "x = np.linspace(0, max(yd_model), 100)\n",
    "sns.lineplot(x, np.multiply(slope, x) + intercept)\n",
    "plt.title(f\"slope={slope:.2f}; r={r:.2f}; p={p:.1e} (model)\")\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('manhattan distance');\n",
    "\n",
    "plt.subplot(224)\n",
    "sns.distplot(wd_model)\n",
    "mu, std = sp.stats.norm.fit(wd_model)\n",
    "x = np.linspace(min(wd_model), max(wd_model), 100)\n",
    "plt.plot(x, sp.stats.norm.pdf(x, mu, std))\n",
    "plt.xlabel('manhattan distance')\n",
    "plt.ylabel('probability distribution');\n",
    "plt.title(f\"μ={mu:.2}, σ={std:.2} (model)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_model = neighbor_similarity(subgraph, vectors)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x=np.abs(yd_model), y=neighbors_model)\n",
    "slope, intercept, r, p, stderr = sp.stats.linregress(np.abs(yd_model), neighbors_model)\n",
    "x = np.linspace(0, max(yd_model), 100)\n",
    "sns.lineplot(x, np.multiply(slope, x) + intercept)\n",
    "plt.title(f\"slope={slope:.2f}; r={r:.2f}; p={p:.1e}\")\n",
    "plt.xlabel('Δyear')\n",
    "plt.ylabel('cosine similarity');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "sns.scatterplot(x='index', y='weight',\n",
    "                data=pd.DataFrame({'index': vectors.indices,\n",
    "                                   'weight': vectors.data}))\n",
    "plt.ylim([-.1,1.1]);\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_distribution(vectors.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "plt.subplot(121)\n",
    "nx.draw(graph)\n",
    "plt.title('original graph')\n",
    "plt.subplot(122)\n",
    "nx.draw(subgraph)\n",
    "plt.title('new graph');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_graph(g, name):\n",
    "    nodes = [{'name': str(i)}#, 'club': 0 #g.node[i]['club']}\n",
    "             for i in g.nodes()]\n",
    "    links = [{'source': list(g.nodes).index(u),\n",
    "              'target': list(g.nodes).index(v)}\n",
    "             for u,v in g.edges()]\n",
    "    with open(name + '.json', 'w') as f:\n",
    "        json.dump({'nodes': nodes, 'links': links},\n",
    "                  f, indent=4,)\n",
    "\n",
    "save_graph(graph, 'graph')\n",
    "save_graph(subgraph, 'subgraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<div id=\"graph\"></div>\n",
    "<style>\n",
    ".node {stroke: #fff; stroke-width: 1.5px;}\n",
    ".link {stroke: #999; stroke-opacity: .6;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<div id=\"subgraph\"></div>\n",
    "<style>\n",
    ".node {stroke: #fff; stroke-width: 1.5px;}\n",
    ".link {stroke: #999; stroke-opacity: .6;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "// We load the d3.js library from the Web.\n",
    "require.config({paths:\n",
    "    {d3: \"http://d3js.org/d3.v3.min\"}});\n",
    "require([\"d3\"], function(d3) {\n",
    "  // The code in this block is executed when the\n",
    "  // d3.js library has been loaded.\n",
    "\n",
    "  // First, we specify the size of the canvas\n",
    "  // containing the visualization (size of the\n",
    "  // <div> element).\n",
    "  var width = 800, height = 400;\n",
    "  var g = 'subgraph';\n",
    "\n",
    "  // We create a color scale.\n",
    "  var color = d3.scale.category10();\n",
    "\n",
    "  // We create a force-directed dynamic graph layout.\n",
    "  var force = d3.layout.force()\n",
    "    .charge(-120)\n",
    "    .linkDistance(50)\n",
    "    .size([width, height]);\n",
    "\n",
    "  // In the <div> element, we create a <svg> graphic\n",
    "  // that will contain our interactive visualization.\n",
    "  var svg = d3.select('#'.concat(g)).select(\"svg\")\n",
    "  if (svg.empty()) {\n",
    "    svg = d3.select('#'.concat(g)).append(\"svg\")\n",
    "          .attr(\"width\", width)\n",
    "          .attr(\"height\", height);\n",
    "  }\n",
    "\n",
    "  // We load the JSON file.\n",
    "  d3.json(g.concat('.json'), function(error, graph) {\n",
    "    // In this block, the file has been loaded\n",
    "    // and the 'graph' object contains our graph.\n",
    "\n",
    "    // We load the nodes and links in the\n",
    "    // force-directed graph.\n",
    "    force.nodes(graph.nodes)\n",
    "      .links(graph.links)\n",
    "      .start();\n",
    "\n",
    "    // We create a <line> SVG element for each link\n",
    "    // in the graph.\n",
    "    var link = svg.selectAll(\".link\")\n",
    "      .data(graph.links)\n",
    "      .enter().append(\"line\")\n",
    "      .attr(\"class\", \"link\");\n",
    "\n",
    "    // We create a <circle> SVG element for each node\n",
    "    // in the graph, and we specify a few attributes.\n",
    "    var node = svg.selectAll(\".node\")\n",
    "      .data(graph.nodes)\n",
    "      .enter().append(\"circle\")\n",
    "      .attr(\"class\", \"node\")\n",
    "      .attr(\"r\", 5)  // radius\n",
    "      .style(\"fill\", function(d) {\n",
    "         // The node color depends on the club.\n",
    "         return color(d.club);\n",
    "      })\n",
    "      .call(force.drag);\n",
    "\n",
    "    // The name of each node is the node number.\n",
    "    node.append(\"title\")\n",
    "        .text(function(d) { return d.name; });\n",
    "\n",
    "    // We bind the positions of the SVG elements\n",
    "    // to the positions of the dynamic force-directed\n",
    "    // graph, at each time step.\n",
    "    force.on(\"tick\", function() {\n",
    "      link.attr(\"x1\", function(d){return d.source.x})\n",
    "          .attr(\"y1\", function(d){return d.source.y})\n",
    "          .attr(\"x2\", function(d){return d.target.x})\n",
    "          .attr(\"y2\", function(d){return d.target.y});\n",
    "\n",
    "      node.attr(\"cx\", function(d){return d.x})\n",
    "          .attr(\"cy\", function(d){return d.y});\n",
    "    });\n",
    "  });\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The point of this model is that one can model knowledge discovery as incremental changes on existing knowledge.\n",
    "\n",
    "The mutation model doesn't monotonically decrease similarity with parent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
